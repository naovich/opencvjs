<!DOCTYPE html>
<html lang="fr">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Détection de visage et contour de carte avec OpenCV.js</title>
    <style>
      body {
        font-family: Arial, sans-serif;
        max-width: 1200px;
        margin: 0 auto;
        padding: 20px;
      }
      .container {
        display: flex;
        gap: 20px;
        margin: 20px 0;
      }
      .video-section {
        flex: 1;
        position: relative;
      }
      .result-section {
        flex: 1;
      }
      .controls {
        margin: 10px 0;
      }
      button {
        padding: 10px 20px;
        font-size: 16px;
        cursor: pointer;
      }
      #overlay-rectangle {
        position: absolute;
        border: 2px solid orange;
        pointer-events: none;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
      }
      .nav-menu {
        background-color: #333;
        padding: 15px 0;
        margin-bottom: 20px;
      }
      .nav-menu ul {
        list-style-type: none;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        gap: 20px;
      }
      .nav-menu a {
        color: white;
        text-decoration: none;
        font-size: 16px;
        padding: 8px 15px;
        border-radius: 4px;
        transition: background-color 0.3s;
      }
      .nav-menu a:hover {
        background-color: #555;
      }
      .nav-menu a.active {
        background-color: #4caf50;
      }
      .result-info {
        margin-top: 10px;
      }
      .metric {
        font-weight: bold;
        color: #333;
      }
      .metric-row {
        margin: 10px 0;
        padding: 5px 0;
        border-bottom: 1px solid #ddd;
      }
    </style>
  </head>
  <body>
    <nav class="nav-menu">
      <ul>
        <li><a href="index.html" class="active">Analyse d'image</a></li>
        <li><a href="face.html">Face recognition</a></li>
        <li>
          <a href="facePlusCardaprox.html">Face et carte</a>
        </li>
        <li><a href="faceEyesCard.html">Face, carte et eyes</a></li>
        <li><a href="faceCardAproxContour.html">Face, carte et contour</a></li>
        <li><a href="faceCardOrientation.html">Face, orientation</a></li>
      </ul>
    </nav>
    <h1>Détection de visage et du contour de la carte</h1>
    <div class="container">
      <div class="video-section">
        <video id="video" width="640" height="480" autoplay></video>
        <div id="overlay-rectangle"></div>
        <div class="controls">
          <button id="toggleOverlay">Afficher/Masquer le guide</button>
        </div>
      </div>
      <div class="result-section">
        <canvas
          id="canvas"
          width="640"
          height="480"
          willReadFrequently="true"
        ></canvas>
        <div class="result-info">
          <div class="metric-row">
            <p>
              Luminosité moyenne:
              <span id="brightnessValue" class="metric">-</span>
            </p>
          </div>
          <div class="metric-row">
            <p>
              Variance (Flou): <span id="varianceValue" class="metric">-</span>
            </p>
          </div>
          <div class="metric-row">
            <p>Statut: <span id="result" class="metric"></span></p>
          </div>
        </div>
      </div>
    </div>

    <script>
      // Variables globales
      let video = document.getElementById("video");
      let canvas = document.getElementById("canvas");
      let context = canvas.getContext("2d");
      let brightnessValue = document.getElementById("brightnessValue");
      let varianceValue = document.getElementById("varianceValue");
      let result = document.getElementById("result");
      let openCv = null;
      let faceClassifier = null;
      let isOverlayVisible = false;

      // Paramètres et constantes
      const ANALYSIS_INTERVAL = 100;
      const VIDEO_WIDTH = 640;
      const VIDEO_HEIGHT = 480;
      const targetRatio = 85.6 / 53.98;
      const margin = 100;
      const rectHeight = VIDEO_HEIGHT - margin * 2.5;
      const rectWidth = rectHeight * targetRatio;
      const ANGLE_TOLERANCE = 5;
      const OVERLAP_THRESHOLD = 0.7;
      const SIZE_DIFFERENCE_THRESHOLD = 0.2;

      // Configuration de l'overlay guide (pour la carte)
      const overlay = document.getElementById("overlay-rectangle");
      overlay.style.width = rectWidth + "px";
      overlay.style.height = rectHeight + "px";
      overlay.style.top = "35%";
      overlay.style.left = "50%";
      overlay.style.transform = "translate(-50%, -50%)";

      // Accès à la caméra
      navigator.mediaDevices
        .getUserMedia({ video: true })
        .then((stream) => {
          video.srcObject = stream;
        })
        .catch((err) => {
          console.error("Erreur lors de l'accès à la caméra:", err);
        });

      // Chargement du cascade classifier pour le visage
      async function loadCascadeClassifier() {
        const response = await fetch(
          "src/classifiers/haarcascade_frontalface_default.xml"
        );
        const buffer = await response.arrayBuffer();
        const data = new Uint8Array(buffer);
        openCv.FS_createDataFile(
          "/",
          "haarcascade_frontalface_default.xml",
          data,
          true,
          false,
          false
        );
        let classifier = new openCv.CascadeClassifier();
        classifier.load("haarcascade_frontalface_default.xml");
        return classifier;
      }

      // ---------------------------
      // Détection du visage
      // ---------------------------
      async function faceDetectionOnFrame(src) {
        let gray = new openCv.Mat();
        openCv.cvtColor(src, gray, openCv.COLOR_RGBA2GRAY);
        if (!faceClassifier) {
          faceClassifier = await loadCascadeClassifier();
        }
        let faces = new openCv.RectVector();
        faceClassifier.detectMultiScale(gray, faces, 1.3, 7, 0);
        // On dessine le premier visage détecté (ou le plus grand)
        let faceRect = null;
        if (faces.size() > 0) {
          let face = faces.get(0);
          faceRect = {
            x: face.x,
            y: face.y,
            width: face.width,
            height: face.height,
          };
          let pt1 = new openCv.Point(face.x, face.y);
          let pt2 = new openCv.Point(face.x + face.width, face.y + face.height);
          openCv.rectangle(src, pt1, pt2, [255, 0, 0, 255], 2);
        }
        gray.delete();
        faces.delete();
        return faceRect;
      }

      // ---------------------------
      // Détection du contour de la carte (algorithme existant)
      // ---------------------------
      function cardDetectionOnFrame(src, faceRect) {
        let gray = new openCv.Mat();
        openCv.cvtColor(src, gray, openCv.COLOR_RGBA2GRAY);
        let blurred = new openCv.Mat();
        openCv.GaussianBlur(gray, blurred, new openCv.Size(5, 5), 0);
        let thresh = new openCv.Mat();
        openCv.adaptiveThreshold(
          blurred,
          thresh,
          255,
          openCv.ADAPTIVE_THRESH_GAUSSIAN_C,
          openCv.THRESH_BINARY,
          11,
          2
        );
        let edges = new openCv.Mat();
        openCv.Canny(blurred, edges, 50, 150);
        let contours = new openCv.MatVector();
        let hierarchy = new openCv.Mat();
        openCv.findContours(
          edges,
          contours,
          hierarchy,
          openCv.RETR_EXTERNAL,
          openCv.CHAIN_APPROX_SIMPLE
        );

        let maxArea = 0;
        let maxContourIndex = -1;
        for (let i = 0; i < contours.size(); i++) {
          let contour = contours.get(i);
          let area = openCv.contourArea(contour);
          if (area > maxArea) {
            maxArea = area;
            maxContourIndex = i;
          }
        }

        if (maxContourIndex >= 0) {
          let maxContour = contours.get(maxContourIndex);
          let approx = new openCv.Mat();
          let epsilon = 0.02 * openCv.arcLength(maxContour, true);
          openCv.approxPolyDP(maxContour, approx, epsilon, true);
          if (approx.rows == 4) {
            console.log("Quadrilatère détecté");
            // Affichage des segments du contour détecté
            let pts = [];
            for (let i = 0; i < 4; i++) {
              let x = approx.data32S[i * 2];
              let y = approx.data32S[i * 2 + 1];
              pts.push(new openCv.Point(x, y));
            }
            // Tracer le contour en vert
            let matVec = new openCv.MatVector();
            matVec.push_back(approx);
            openCv.polylines(
              src,
              matVec,
              true,
              new openCv.Scalar(0, 255, 0, 255),
              2
            );
            matVec.delete();
          } else {
            console.log("Contour détecté, mais ce n'est pas un quadrilatère");
          }
          approx.delete();
        } else {
          console.log("Aucun contour significatif détecté");
        }
        gray.delete();
        blurred.delete();
        thresh.delete();
        edges.delete();
        hierarchy.delete();
        contours.delete();
      }

      // ---------------------------
      // Analyse de luminosité et variance (flou)
      // ---------------------------
      async function analyseBrightness(gray) {
        let meanStdDev = new openCv.Mat();
        let mean = new openCv.Mat();
        openCv.meanStdDev(gray, mean, meanStdDev);
        let brightness = Math.round(mean.data64F[0]);
        brightnessValue.textContent = brightness;
        mean.delete();
        meanStdDev.delete();
      }

      async function analyseVariance(gray) {
        let laplacian = new openCv.Mat();
        let mean = new openCv.Mat();
        let stdDev = new openCv.Mat();
        openCv.Laplacian(gray, laplacian, openCv.CV_64F);
        openCv.meanStdDev(laplacian, mean, stdDev);
        let variance = Math.round(stdDev.data64F[0] * stdDev.data64F[0]);
        varianceValue.textContent = variance;
        laplacian.delete();
        mean.delete();
        stdDev.delete();
      }

      // ---------------------------
      // Processus principal sur le frame capturé
      // ---------------------------
      async function processFrame() {
        // Capture de l'image depuis la vidéo dans le canvas
        context.drawImage(video, 0, 0, VIDEO_WIDTH, VIDEO_HEIGHT);
        let src = openCv.imread(canvas);

        // Détection du visage et récupération de son rectangle
        let faceRect = await faceDetectionOnFrame(src);

        // Détection du contour de la carte
        cardDetectionOnFrame(src, faceRect);

        // Affichage du résultat dans le canvas via openCv.imshow
        openCv.imshow(canvas, src);

        // Pour les analyses de luminosité et de variance
        let gray = new openCv.Mat();
        openCv.cvtColor(src, gray, openCv.COLOR_RGBA2GRAY);
        await analyseBrightness(gray);
        await analyseVariance(gray);

        src.delete();
        gray.delete();
      }

      async function runAllAnalyses() {
        if (!openCv) return;
        await processFrame();
      }

      async function onOpenCvReady() {
        console.log("OpenCV.js est prêt.");
        openCv = await cv;
        if (!faceClassifier) {
          faceClassifier = await loadCascadeClassifier();
        }
        setInterval(runAllAnalyses, ANALYSIS_INTERVAL);
      }

      document.getElementById("toggleOverlay").addEventListener("click", () => {
        overlay.style.display =
          overlay.style.display === "none" || overlay.style.display === ""
            ? "block"
            : "none";
      });
    </script>
    <script
      async
      src="https://docs.opencv.org/4.11.0/opencv.js"
      onload="onOpenCvReady();"
    ></script>
  </body>
</html>
